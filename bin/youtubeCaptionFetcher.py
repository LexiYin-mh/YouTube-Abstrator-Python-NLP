# This class is used for fetching the captions of a youtube video
# It uses the youtube api to fetch the captions

from youtube_transcript_api import YouTubeTranscriptApi

# retrieve the available transcripts
# transcript_list = YouTubeTranscriptApi.list_transcripts('cuOxGMoHMMY')
transcript_list = YouTubeTranscriptApi.list_transcripts('3Rzi11Hvyh0')
paragraph = ''

# iterate over all available transcripts
for transcript in transcript_list:

    # the Transcript object provides metadata properties
    # print(
    #    transcript.video_id,
    #    transcript.language,
    #    transcript.language_code,
    #    # whether it has been manually created or generated by YouTube
    #    transcript.is_generated,
    #    # whether this transcript can be translated or not
    #    transcript.is_translatable,
    #    # a list of languages the transcript can be translated to
    #    transcript.translation_languages,
    # )

    if transcript.language_code == 'en':
        # fetch the actual transcript data
        # print(transcript.fetch())

        for transcript_sentence in transcript.fetch():
            sentence = transcript_sentence['text']
            paragraph += sentence + '\n'

        paragraph = paragraph.replace('\n', ' ')
        paragraph = paragraph.replace('. ', '.\n\n')

        # print(paragraph)

        break

    # translating the transcript will return another transcript object
    # print(transcript.translate('en').fetch())

# you can also directly filter for the language you are looking for, using the transcript list
# transcript = transcript_list.find_transcript(['en'])

# or just filter for manually created transcripts
# transcript = transcript_list.find_manually_created_transcript(['en'])

# or automatically generated ones
# transcript = transcript_list.find_generated_transcript(['en'])

import tensorflow as tf
import tokenizer
import numpy as np
import evaluation

_VOCAB_FILE = 'ckpt/c4.unigram.newline.10pct.96000.model'

encoder = tokenizer.generate_text_encoder("sentencepiece",
                                                    _VOCAB_FILE)
input_size_limit = {
    'cnn_dailymail': 1024
}

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description="Process model and article related arguments.")

    parser.add_argument("--model_dir", help="path of your model", default="model/")

    parser.add_argument("--model_name", help="name of your model", default="cnn_dailymail")

    args = parser.parse_args()

    print(paragraph)

    input_ids = encoder.encode(paragraph)

    print(input_ids)

    idx = len(input_ids)
    if idx > input_size_limit[args.model_name]:
        idx = input_size_limit[args.model_name]

    inputs = np.zeros(input_size_limit[args.model_name])
    inputs[:idx] = input_ids[:idx]

    imported = tf.saved_model.load(args.model_dir, tags='serve')
    # Initialize a TF example
    example = tf.train.Example()
    # Load inputs into example feature dictionary
    example.features.feature["inputs"].int64_list.value.extend(inputs.astype(int))
    # Generate outputs by signature serving_default
    output = imported.signatures['serving_default'](examples=tf.constant([example.SerializeToString()]))

    summary = evaluation.ids2str(encoder, output['outputs'].numpy(), None)

    print(summary)


